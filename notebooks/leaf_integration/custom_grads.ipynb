{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch supports modifying the gradient of a tensor by registring a hook, this may be the simpliest approach\n",
    "to integrating the leaf classification network. \n",
    "\n",
    "The goal is to add linearly the leaf classification term to the existing cross entropy loss. The leaf classification term\n",
    "will be non-differentiable with respect to the parameters for a few reasons:\n",
    "\n",
    "We can apply the leaf classifier to the output of the semantic segmentation model by\n",
    "- Grouping points by cluster\n",
    "- Down sampling to 80 points\n",
    "- Executing the leaf classification model on each cluster\n",
    "- Adding 1 to the loss for the point if clustered correctly\n",
    "- Removing 1 from the loss if clustered incorrectly\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{leaf} = \\frac{\\sum_k N_k * C(ds(X_k), ds(Y_k), ds(Z_k))}{N_{total}}\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{aug} = \\mathcal{L}(w, b | X, Y_{act}, Y_{pred}) +  \\mathcal{L}_{leaf}(X, Y_{act}, Y_{pred})\n",
    "$$\n",
    "\n",
    "1. In the current implementation of the leaf classifier the points for a particular leaf cluster must be downsampled to apply the leaf classifier.\n",
    "2. Even if not down-sampled the leaf classifier must be applied to a group of points. The grouping operation itself is non-differentiable \n",
    "\n",
    "The gradient update step would be \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W' &= W - \\eta \\nabla \\mathcal{L}_{aug}(W | X, Y_{act}, Y_{pred}) \\\\\n",
    "   &= W - \\eta (\\nabla \\mathcal{L}_{ce}(W | X, Y_{act}, Y_{pred}) + \\nabla \\mathcal{L}_{leaf}(W | X, Y_{act}, Y_{pred}))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However this term:\n",
    "\n",
    "$$\n",
    "\\nabla \\mathcal{L}_{leaf}(W | X, Y_{act}, Y_{pred})\n",
    "$$\n",
    "\n",
    "Has no explicit expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 4, 5],\n",
       "         [5, 6, 7]],\n",
       "\n",
       "        [[4, 5, 6],\n",
       "         [8, 1, 9]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of gather operation usage\n",
    "\n",
    "import torch\n",
    "\n",
    "t = torch.tensor([\n",
    "  [\n",
    "    [1, 2, 3], \n",
    "    [3, 4, 5], \n",
    "    [5, 6, 7]\n",
    "  ], \n",
    "  [\n",
    "    [4, 5, 6],\n",
    "    [8, 1, 9],\n",
    "    [2, 1, 2]\n",
    "  ]\n",
    "])\n",
    "\n",
    "torch.gather(t, 1, torch.tensor([\n",
    "  [\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]\n",
    "  ],\n",
    "  [ \n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1]\n",
    "  ]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [4, 4, 4],\n",
       "        [6, 6, 6]],\n",
       "\n",
       "       [[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [4, 4, 4]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of repeat operation usage\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "test = np.array([[0, 4, 6], [1, 2, 4]])\n",
    "test_expanded = np.expand_dims(test, axis=2)\n",
    "\n",
    "np.repeat(test_expanded, 3, axis=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.  3.  6. 15. 10. 12.  1.  9.]\n",
      " [15.  0. 13. 18. 17. 16.  5. 14.]\n",
      " [14. 12. 15. 11. 13.  9. 18.  5.]\n",
      " [ 1.  9. 16.  2. 19. 18. 11. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# utility function to generate rows of random choice vectors\n",
    "def multi_random_choice(samples, s_size, max):\n",
    "\n",
    "  out = np.zeros((samples, s_size))\n",
    "\n",
    "  for i in range(samples):\n",
    "    out[i,:] = np.random.choice(max, s_size, replace=False)\n",
    "\n",
    "  return out\n",
    "\n",
    "print(multi_random_choice(4, 8, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather_idx:  tensor([[[0, 0, 0],\n",
      "         [4, 4, 4],\n",
      "         [6, 6, 6]],\n",
      "\n",
      "        [[1, 1, 1],\n",
      "         [2, 2, 2],\n",
      "         [4, 4, 4]]])\n",
      "src:  tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "result:  tensor([[[1., 1., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [1., 1., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [1., 1., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Example of gradient upsampling operation\n",
    "\n",
    "test_grad = np.ones((2, 3, 3)) \n",
    "\n",
    "\n",
    "test = np.array([[0, 4, 6], [1, 2, 4]])\n",
    "test_expanded = np.expand_dims(test, axis=2)\n",
    "\n",
    "gather_idx = np.repeat(test_expanded, 3, axis=2 )\n",
    "gather_idx = torch.tensor(gather_idx, dtype=torch.int64)\n",
    "\n",
    "test_input = np.random.choice(6, (2, 7, 3))\n",
    "\n",
    "grad_out = torch.zeros(test_input.shape, dtype=torch.float32)\n",
    "\n",
    "src = torch.tensor(test_grad, dtype=torch.float32) \n",
    "print(\"gather_idx: \", gather_idx)\n",
    "print(\"src: \", src)\n",
    "print(\"result: \", grad_out.scatter_(1, gather_idx, src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of downsample:  tensor([[[1., 2., 3.],\n",
      "         [6., 6., 4.]],\n",
      "\n",
      "        [[3., 2., 4.],\n",
      "         [8., 1., 9.]]], grad_fn=<DownsampleBackward>)\n",
      "result of sum:  tensor(49., grad_fn=<SumBackward0>)\n",
      "gradient, d test / d result of sum:  tensor([[[1., 1., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [1., 1., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/work/murph186/repos\")\n",
    "sys.path.append(\"/work/murph186/repos/TreePartNet/\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Function\n",
    "from SorghumPartNet.models.extensions import Downsample\n",
    "\n",
    "ds = Downsample.apply\n",
    "\n",
    "test = torch.tensor([\n",
    "  [\n",
    "    [1, 2, 3], \n",
    "    [3, 4, 5], \n",
    "    [5, 6, 7],\n",
    "    [6, 6, 4]\n",
    "  ], \n",
    "  [\n",
    "    [4, 5, 6],\n",
    "    [8, 1, 9],\n",
    "    [2, 1, 2],\n",
    "    [3, 2, 4]\n",
    "  ]\n",
    "], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "\n",
    "o = ds(test, 2)\n",
    "\n",
    "print(\"result of downsample: \", o)\n",
    "\n",
    "result = o.sum()\n",
    "\n",
    "print(\"result of sum: \", result)\n",
    "\n",
    "result.backward()\n",
    "print(\"gradient, d test / d result of sum: \", test.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000, 4.5000],\n",
       "        [4.5000, 4.5000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of using .grad property\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(out)\n",
    "\n",
    "out.backward()\n",
    "\n",
    "x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spartnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcd6d3ebe9076c7f55953676c4608cb3132b0a6dbb3127ca91c74f963ad52978"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
