{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of point and feature base clustering approaches, specifically the distance thresholding and mean shift clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import operator\n",
    "from torch import exp, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of threshold method\n",
    "\n",
    "def threshold_cluster(data, threshold):\n",
    "  data.cuda()\n",
    "  distance_pred = torch.cdist(data, data)\n",
    "\n",
    "  ones = torch.ones(distance_pred.shape).cuda()\n",
    "  zeros = torch.zeros(distance_pred.shape).cuda()\n",
    "\n",
    "  distance_pred = torch.where((distance_pred < threshold).cuda(), ones, zeros).cpu().detach().numpy()\n",
    "\n",
    "  cluster_assignments = np.zeros(distance_pred.shape[0], dtype=np.int64)\n",
    "  next_label = 1\n",
    "\n",
    "  for i in range(distance_pred.shape[0]):\n",
    "      if cluster_assignments[i] == 0:\n",
    "        cluster_assignments[i] = next_label\n",
    "        next_label += 1\n",
    "\n",
    "        ind = np.where(distance_pred[i] == 1)\n",
    "\n",
    "        for j in ind[0]:\n",
    "            cluster_assignments[j] = cluster_assignments[i]\n",
    "\n",
    "  return cluster_assignments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of mean shift method\n",
    "\n",
    "def distance_batch(a, b):\n",
    "    return sqrt(((a[None,:] - b[:,None]) ** 2).sum(2))\n",
    "\n",
    "def gaussian(dist, bandwidth):\n",
    "  return exp(-0.5 * ((dist / bandwidth))**2) / (bandwidth * math.sqrt(2 * math.pi))\n",
    "\n",
    "# sourced from https://colab.research.google.com/github/sotte/pytorch_tutorial/blob/master/notebooks/mean_shift_clustering.ipynb#scrollTo=g0rJs_0BeVSB\n",
    "# Assigns a likely cluster center for each input point\n",
    "def meanshift_torch(data, batch_size=500, window_size=0.5, steps=10):\n",
    "    n = len(data)\n",
    "    X = data.cuda()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        for i in range(0, n, batch_size):\n",
    "            s = slice(i, min(n, i + batch_size))\n",
    "            weight = gaussian(distance_batch(X, X[s]), window_size)\n",
    "            num = (weight[:, :, None] * X).sum(dim=1)\n",
    "            X[s] = num / weight.sum(1)[:, None]\n",
    "\n",
    "    return X\n",
    "\n",
    "# Applies mean shift clustering to predict cluster centers, then uses \n",
    "# thresholding to return cluster assignments for each point\n",
    "def meanshift_cluster(data, batch_size=10, threshold=1, window_size=1, steps=15):\n",
    "  X = meanshift_torch(data, batch_size, window_size, steps)\n",
    "  cluster_assignments = threshold_cluster(X, threshold)\n",
    "  return cluster_assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw cluster:  tensor([[ 1.0500,  2.1000,  3.1500,  4.0000],\n",
      "        [ 1.0500,  2.1000,  3.1500,  4.0000],\n",
      "        [ 7.0500,  7.0500,  7.2500,  8.0000],\n",
      "        [ 7.0500,  7.0500,  7.2500,  8.0000],\n",
      "        [12.0000, 12.0000, 12.0000, 11.0000]], device='cuda:0')\n",
      "Cluster Assign [1 1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Example of mean-shift cluster usage\n",
    "\n",
    "# Define some test points\n",
    "test = [[1,2,3,4], [1.1, 2.2, 3.3,4], [7, 7, 7, 8], [7.1, 7.1, 7.5, 8], [12, 12, 12,11]]\n",
    "\n",
    "print(\"Raw cluster: \", meanshift_torch(torch.tensor(test)))\n",
    "print(\"Cluster Assign\", meanshift_cluster(torch.tensor(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Model input shape: torch.Size([8000, 3])\n",
      "Feature Vectors Shape:  torch.Size([8000, 256])\n"
     ]
    }
   ],
   "source": [
    "# Apply each clustering to the DGCNN output for our trained model\n",
    "# We first load the model and a test sample\n",
    "\n",
    "sys.path.append(\"/work/murph186/repos\")\n",
    "sys.path.append(\"/work/murph186/repos/TreePartNet/\")\n",
    "\n",
    "# Configuration\n",
    "model_name = \"SorghumPartNetInstance\"\n",
    "version = 0\n",
    "model_checkpoint_path = f\"/space/ariyanzarei/sorghum_segmentation/models/model_checkpoints/{model_name}/lightning_logs/version_{version}/checkpoints/epoch=9-step=9379.ckpt\" \n",
    "test_dataset_path = \"/space/ariyanzarei/sorghum_segmentation/dataset/2022-03-10/sorghum__labeled_test.hdf5\"\n",
    "test_index = 2\n",
    "\n",
    "from SorghumPartNet.models.nn_models import SorghumPartNetInstance \n",
    "model = SorghumPartNetInstance.load_from_checkpoint(model_checkpoint_path).cuda()\n",
    "\n",
    "from SorghumPartNet.train_and_inference.predict_and_visualize import load_test_data\n",
    "import torch\n",
    "\n",
    "test_points,_,_,plant_index,leaf_index = load_test_data(test_dataset_path, test_index)\n",
    "\n",
    "print(f\"Model input shape: {test_points.shape}\")\n",
    "\n",
    "pred_instance_features = model(torch.unsqueeze(test_points,dim=0).cuda()).detach()\n",
    "pred_instance_features = torch.squeeze(pred_instance_features)\n",
    "\n",
    "print(\"Feature Vectors Shape: \", pred_instance_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh clusters: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109] [5501    4   93    6   13    7    8    6   66  109  130    3   16   59\n",
      "   72   25    4   72   24   32   38   42    5    4   47   15  106   60\n",
      "   69  109    4   72   70   19   43   19   80   26   17    2    5   12\n",
      "   16   75    8    7   36   52   18   24    8   12   30    4   93   29\n",
      "    4   10    1   22   27    5    2   60   16   10    8   16   27    3\n",
      "    1    5    2    1   11    2    1    4    3   27    7   16    2   15\n",
      "   10    3   24    3    6    1    6   25    1    2    4    1   15   10\n",
      "    8    3    4   12    1    3    2    7    1    1   13]\n",
      "Total found:  [  1   3   5   9  10  11  13  14  15  16  18  19  20  21  22  25  26  27\n",
      "  28  29  30  32  33  34  35  36  37  38  39  42  43  44  47  48  49  50\n",
      "  52  53  55  56  60  61  64  65  68  69  75  80  82  84  87  92  97 102\n",
      " 109] (55,)\n"
     ]
    }
   ],
   "source": [
    "# Application of thresholding\n",
    "\n",
    "thresh_cluster_assign = threshold_cluster(pred_instance_features, 5)\n",
    "thresh_cluster_labels, cluster_counts = np.unique(thresh_cluster_assign, return_counts=True)\n",
    "\n",
    "print(\"Thresh clusters:\", thresh_cluster_labels, cluster_counts)\n",
    "\n",
    "count_threshold = 10\n",
    "\n",
    "thresh_filtered_clusters = thresh_cluster_labels[cluster_counts > count_threshold]\n",
    "\n",
    "print(\"Total found: \", thresh_filtered_clusters, thresh_filtered_clusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meanshift clusters: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] [5502  113  184  256  168   57   95  196  220   16  115   82   72   17\n",
      "   81   70  115   50   18  109   33  155  129   43   26    4   29    4\n",
      "    5    6   16    2    1    4    1    1    1    3    1]\n",
      "Total found:  [ 1  2  3  4  5  6  7  8  9 11 12 13 15 16 17 18 20 21 22 23 24 25 27] (23,)\n"
     ]
    }
   ],
   "source": [
    "# Applicaiton of Mean-shift\n",
    "meanshift_cluster_assign = meanshift_cluster(pred_instance_features, threshold=0.6, steps=40, window_size=1.5)\n",
    "\n",
    "meanshift_cluster_labels, cluster_counts = np.unique(meanshift_cluster_assign, return_counts=True)\n",
    "\n",
    "print(\"Meanshift clusters:\", meanshift_cluster_labels, cluster_counts)\n",
    "\n",
    "count_threshold = 20\n",
    "\n",
    "meanshift_filtered_clusters = meanshift_cluster_labels[cluster_counts > count_threshold]\n",
    "\n",
    "print(\"Total found: \", meanshift_filtered_clusters, meanshift_filtered_clusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets generate the feature vector with the model and compare the result of meanshift to the existing clustering approach\n",
    "\n",
    "import k3d\n",
    "\n",
    "def plot_results(cluster_assign, filtered_clusters):\n",
    "  colors = [0xe41a1c,0x377eb8,0x4daf4a,0x984ea3,0xff7f00,0xffff33,0xa65628,0xf781bf,0x999999]\n",
    "\n",
    "  plot = k3d.plot(name='points')\n",
    "  filtered_list = []\n",
    "  for i, c in enumerate(np.unique(cluster_assign)):\n",
    "\n",
    "    if c in filtered_clusters:\n",
    "      cluster_points = test_points[cluster_assign == c]\n",
    "\n",
    "      color = colors[i % len(colors)] \n",
    "      plt_points = k3d.points(positions=cluster_points, point_size=0.01, color=color, name=f\"class {c}\")\n",
    "      plot += plt_points\n",
    "    else:\n",
    "      filtered_list.append(test_points[cluster_assign == c])\n",
    "\n",
    "  filtered_points = np.concatenate(filtered_list) \n",
    "  plt_points = k3d.points(positions=filtered_points, point_size=0.01, color=0xe0e0e0, name=\"other\")\n",
    "  plot += plt_points\n",
    "\n",
    "  plot.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78b5891163547e38201167e83ba563d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(thresh_cluster_assign, thresh_filtered_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d549d488168e42b3912bf76c6421b1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(meanshift_cluster_assign, meanshift_filtered_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold Rand Score:  0.9888163742917845\n",
      "Meanshift Rand Score:  0.9864319659188563\n",
      "[[ 0  0  1 ...  0  0  0]\n",
      " [ 0  0 82 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "# Here we attempt to compare relevant metrics between the clusters derived \n",
    "# from the threshodl method and the mean-shift method\n",
    "\n",
    "def get_test_labels(leaf_index, plant_index):\n",
    "  return list(set(list(zip(leaf_index.detach().cpu().numpy(), plant_index.detach().cpu().numpy()))))\n",
    "\n",
    "# This is needed because leaf indicies can be duplicated with multiple plants in a single\n",
    "# sample\n",
    "label_lookup = get_test_labels(leaf_index, plant_index)\n",
    "test_labels = [ label_lookup.index((leaf_index[i], plant_index[i])) for i in range(len(leaf_index))]\n",
    "\n",
    "\n",
    "from sklearn.metrics.cluster import rand_score, adjusted_rand_score, contingency_matrix\n",
    "\n",
    "threshold_rand_score = adjusted_rand_score(test_labels, thresh_cluster_assign)\n",
    "meanshift_rand_score = adjusted_rand_score(test_labels, meanshift_cluster_assign)\n",
    "\n",
    "print(\"Threshold Rand Score: \", threshold_rand_score)\n",
    "print(\"Meanshift Rand Score: \", meanshift_rand_score)\n",
    "\n",
    "print(contingency_matrix(test_labels, meanshift_cluster_assign))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spartnet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcd6d3ebe9076c7f55953676c4608cb3132b0a6dbb3127ca91c74f963ad52978"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
